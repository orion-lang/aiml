[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-ML Journal",
    "section": "",
    "text": "Welcome to my Notes. Browse topics like Classifier, SVD, Markov, and more as I add them.\n\n\n\nMarina"
  },
  {
    "objectID": "classifier.html",
    "href": "classifier.html",
    "title": "My Notes",
    "section": "",
    "text": "0.1 Linear Classifier\nHow to classify various data points into two types ( for example : spam/non spam emails):\n\n\n\nFig1: Binary Classifier\n\n\nHow to identify equation of a classifier (straight line in 2D plane) which can split this data sets in two groups?\nVector Algebra: To help us find distances\n\n\n\nFig2. Line classifier useful in 2D.\n\n\n\n\n\nFig 3: Concept of distance from Classifier\n\n\nThe classification is done by identifying a line in case of 2d which is evenly placed between two groups of data.And, to be specific its determined by calculating the distance from the $ \\color{green} $ line which is the classifier( in fig 3)\nFor example, we will calculate the distance D1, D2, D3 from the classifier line. The magnitude of this distance is used to determine how confident the classifier is about the point. If the distance is positive then the classifier marks them with 1 ( Points \\color{magenta}1 and \\color{magenta}{2}) and if the distance is negative then classifier marks them -1 ( point \\color{blue}3)\n\n\n\nFig4 Orthogonal Projection\n\n\nLet’s assume the plane P is the hyperplane which will be used to classify two groups of data. x is any vector\nDistance between point x and the hyperplane is given by \\alpha\\theta. Why?\nTheta is the Perpendicular to the plane. So the vector to point x is given by \\alpha\\theta\nWe need to find this \\alpha\\theta.\nBy vector geometry :\nw+z=x 1\nEquation of the plane is given by \\theta.x=0 where x is give by:\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n....\\\\\nx_n\n\\end{bmatrix}\nFrom 1 :\nz = x-w\nor:\n\n\\theta^T.z = \\theta^T.x-\\theta^T.w [Dot product with \\theta on both sides: ]\n\nor, z= \\frac{-w.\\theta}{||\\theta||}\n\n(the -ve sign indicates which direction is the distance)\n\n\n0.2 Algorithm: Perceptron\nBinary classification : Data represented by two labels.\nAssume we have a data set as per the table. So we have a feature matrix :\nFeature Matrix\n\n\\begin{bmatrix}\nx_{11} && x_{12} && x_{13} \\\\\nx_{21} && x_{22} && x_{23} \\\\\nx_{31} && x_{32} && x_{33} \\\\\nx_{41} && x_{42} && x_{43} \\\\\nx_{51} && x_{52} && x_{53} \\\\\n\\end{bmatrix}\n\nLabel Matrix\n\ny=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n-1 \\\\\n-1 \\\\\n-1 \\\\\n\\end{bmatrix}\n\n\n\n\n\n\n\nNoteSpecial CallOut\n\n\n\nHello\n\n\n\n\n\n\n\n\n\n\n\n\nLabel\nLabel Value\nFeature\nFeature\nFeature\n\n\n\n\n\n\n\\color{green}1\n\\color{green}2\n\\color{green}3\n\n\ny_{\\color{orange}{1}}\n\\color{deeppink}1\nx_{\\color{green}1}^{\\color{orange}{(1)}}\nx_{\\color{green}2}^{\\color{orange}{(1)}}\nx_{\\color{green}3}^{\\color{orange}{(1)}}\n\n\ny_{\\color{orange}{2}}\n\\color{deeppink}1\nx_{\\color{green}1}^{\\color{orange}{(2)}}\nx_{\\color{green}2}^{\\color{orange}{(2)}}\nx_{\\color{green}3}^{\\color{orange}{(3)}}\n\n\ny_{\\color{orange}{3}}\n\\color{deeppink}1\nx_{\\color{green}1}^{\\color{orange}{(3)}}\nx_{\\color{green}2}^{\\color{orange}{(3)}}\nx_{\\color{green}3}^{\\color{orange}{(3)}}\n\n\ny_{\\color{orange}{4}}\n\\color{blue}{-1}\nx_{\\color{green}1}^{\\color{orange}{(4)}}\nx_{\\color{green}2}^{\\color{orange}{(4)}}\nx_{\\color{green}3}^{\\color{orange}{(4)}}\n\n\ny_{\\color{orange}{5}}\n\\color{blue}{-1}\nx_{\\color{green}1}^{\\color{orange}{(5)}}\nx_{\\color{green}2}^{\\color{orange}{(5)}}\nx_{\\color{green}3}^{\\color{orange}{(5)}}\n\n\n\n\n0.2.1 Pseudocode\n\nInitialise theta and theta_0:\nFor i in range (featire_matrix,shape[0]): iteratively check if the distance is positive distance\n\ndef perceptron_single_step_update( feature_vector, label, current_theta, current_theta_0): \nrows = feature_vector.shape[0]\ndecision = label*(np.dot(current_theta, feature_vector)+current_theta_0)\n\nif decision &lt;= 0:\n    current_theta = current_theta + label*feature_vector\n    current_theta_0 = current_theta_0 + label\n    return (current_theta, current_theta_0)\nelse:\n    return (current_theta, current_theta_0)\n\n\ndef perceptron(feature_matrix, labels, T):\n    theta = np.zeros(feature_matrix.shape[1])\n    theta_0 = 0\n    nsamples = feature_matrix.shape[0]\n    \n    for t in range(T):\n        for i in get_order(nsamples):\n            # Your code here\n            theta, theta_0 = perceptron_single_step_update(\n                feature_matrix[i], labels[i], theta, theta_0)\n            # print(\"current theta\",theta)\n    return (theta, theta_0)\n    \n\n\n\n0.3 Perceptron Performance\nM times the update has been made. We know that the update formula is :\n\\theta_i = \\theta_{i-1} + y_i.x_i x_i = feature vector at ith round\nLet t_i be the times when the weights are updated. t_i \\in [ 1,.....M]\\in [1....T]\nto simplify two weights updates will be indicated by w_{k+1} , and previous one will be indicated by w_k\nTotal Update is Sum of Update at every step:\n\nProof\n\n\n\n\n\n\n\nDeclaration\nLogic\nCol3\n\n\n\n\nSum of Changes=\n\n\n\n\n||\\sum_{t=1}^M y_t.x_t ||=\n\n\n\n\n|| \\sum_1^M  w_{k+1}-w_{k} ||=\nTheta ( or weight changes at each step)\n\n\n\n= ||\\color{red} w_{M+1} - w_{M} +\n\\color{green} w_{M} - w_{M-1} +\n\\color{magenta} w_{M-1} - w_{M-2} +\n………….\n\\color{blue} w_{3} - w_{2} +\n\\color{Aquamarine} w_{2} - w_{1} ||\nAdding individual content\n\n\n\n= ||w_{M+1} - w_1||\nBy telescoping\nw_m is a vector\n\n\n\nw_M="
  }
]