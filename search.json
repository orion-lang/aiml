[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI-ML Journal",
    "section": "",
    "text": "Welcome to my Notes. Browse topics like Classifier, SVD, Markov, and more as I add them.\n\n\n\nMarina"
  },
  {
    "objectID": "classifier.html",
    "href": "classifier.html",
    "title": "My Notes",
    "section": "",
    "text": "0.1 Linear Classifier\nHow to classify various data points into two types ( for example : spam/non spam emails):\n\n\n\nFig1: Binary Classifier\n\n\nHow to identify equation of a classifier (straight line in 2D plane) which can split this data sets in two groups?\nA quick detour with Vector Algebra :\n\n\n\n\n\n\nFig2. Line classifier useful in 2D\n\n\n\n\n\n\n\nFig 3. Distance as aid for classifier\n\n\n\n\n\n\n\n\nFig4 Orthogonal Projection\n\n\nLet’s assume the plane P is the hyperplane which will be used to classify two groups of data. x is any vector. Distance between point x and the hyperplane is given by \\alpha\\theta. Why? Theta is the Perpendicular to the plane. So the vector to point x is given by \\alpha\\theta\nWe need to find this \\alpha\\theta. By vector geometry :\nw+z=x\nEquation of the plane is given by \\theta.x=0 where x is given by:\n\nx= \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n....\\\\\nx_n\n\\end{bmatrix} \\tag{0}\n\n\nz=x-w \\tag{1}\n \n\\theta^T.z = \\theta^T.x-\\theta^T.w\n\\tag{2}\n z= $$\nLarge Margin Classifier\n\n\n0.2 Algorithm: Perceptron\nBinary classification : Data represented by two labels.\nAssume we have a data set as per the table. So we have a feature matrix :\n Feature = \\quad\n\\begin{bmatrix}\nx_{11} && x_{12} && x_{13} \\\\\nx_{21} && x_{22} && x_{23} \\\\\nx_{31} && x_{32} && x_{33} \\\\\nx_{41} && x_{42} && x_{43} \\\\\nx_{51} && x_{52} && x_{53} \\\\\n\\end{bmatrix}\n        \\quad \\quad Label \\quad    y=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n-1 \\\\\n-1 \\\\\n-1 \\\\\n\\end{bmatrix}\n\n\nDate Table\n\n\n\n\n\n\n\n\n\nLabel\nLabel\nFeature\nFeaturey\nFeature\n\n\n\n\n\n\n\\color{green}1\n\\color{green}2\n\\color{green}3\n\n\ny_{\\color{orange}{1}}\n\\color{deeppink}1\nx_{\\color{green}1}^{\\color{orange}{(1)}}\nx_{\\color{green}2}^{\\color{orange}{(1)}}\nx_{\\color{green}3}^{\\color{orange}{(1)}}\n\n\ny_{\\color{orange}{2}}\n\\color{deeppink}1\nx_{\\color{green}1}^{\\color{orange}{(2)}}\nx_{\\color{green}2}^{\\color{orange}{(2)}}\nx_{\\color{green}3}^{\\color{orange}{(3)}}\n\n\ny_{\\color{orange}{3}}\n\\color{deeppink}1\nx_{\\color{green}1}^{\\color{orange}{(3)}}\nx_{\\color{green}2}^{\\color{orange}{(3)}}\nx_{\\color{green}3}^{\\color{orange}{(3)}}\n\n\ny_{\\color{orange}{4}}\n\\color{blue}{-1}\nx_{\\color{green}1}^{\\color{orange}{(4)}}\nx_{\\color{green}2}^{\\color{orange}{(4)}}\nx_{\\color{green}3}^{\\color{orange}{(4)}}\n\n\ny_{\\color{orange}{5}}\n\\color{blue}{-1}\nx_{\\color{green}1}^{\\color{orange}{(5)}}\nx_{\\color{green}2}^{\\color{orange}{(5)}}\nx_{\\color{green}3}^{\\color{orange}{(5)}}\n\n\n\n\n\n\n\n\n  \n    \n      \n        Accordion Item #1\n      \n    \n    \n      \n        This is the first item's accordion body. It is shown by default, until the collapse plugin adds the appropriate classes that we use to style each element. These classes control the overall appearance, as well as the showing and hiding via CSS transitions. You can modify any of this with custom CSS or overriding our default variables. It's also worth noting that just about any HTML can go within the .accordion-body, though the transition does limit overflow.\n      \n    \n  \n  \n    \n      \n        Accordion Item #2\n      \n    \n    \n      \n        This is the second item's accordion body. It is hidden by default, until the collapse plugin adds the appropriate classes that we use to style each element. These classes control the overall appearance, as well as the showing and hiding via CSS transitions. You can modify any of this with custom CSS or overriding our default variables. It's also worth noting that just about any HTML can go within the .accordion-body, though the transition does limit overflow.\n      \n    \n  \n  \n    \n      \n        Accordion Item #3\n      \n    \n    \n      \n        This is the third item's accordion body. It is hidden by default, until the collapse plugin adds the appropriate classes that we use to style each element. These classes control the overall appearance, as well as the showing and hiding via CSS transitions. You can modify any of this with custom CSS or overriding our default variables. It's also worth noting that just about any HTML can go within the .accordion-body, though the transition does limit overflow.\n      \n    \n  \n\n\n\n\n\n\n\n\nNotePseudocode\n\n\n\nUpdate theta once the distance calculated is less than 0.\n\n\n#| code-fold: true\ndef perceptron_single_step_update( feature_vector, label, current_theta, current_theta_0): \nrows = feature_vector.shape[0]\ndecision = label*(np.dot(current_theta, feature_vector)+current_theta_0)\n\nif decision &lt;= 0:\n    current_theta = current_theta + label*feature_vector\n    current_theta_0 = current_theta_0 + label\n    return (current_theta, current_theta_0)\nelse:\n    return (current_theta, current_theta_0)\n\n\ndef perceptron(feature_matrix, labels, T):\n    theta = np.zeros(feature_matrix.shape[1])\n    theta_0 = 0\n    nsamples = feature_matrix.shape[0]\n    \n    for t in range(T):\n        for i in get_order(nsamples):\n            # Your code here\n            theta, theta_0 = perceptron_single_step_update(\n                feature_matrix[i], labels[i], theta, theta_0)\n            # print(\"current theta\",theta)\n    return (theta, theta_0)\n    \n\n\n0.3 Large Margin Classifier\n\n\n\nFig8: Wide margin classifier\n\n\nThe decision boundary equation is given by:\n\n\\theta^T.x +\\theta_0 = 0 \\tag{4}\n\nThe two margins are given by :\n\n\\theta^T.x +\\theta_0 = -1 \\tag{5}\n\n\n\\theta^T.x +\\theta_0 = +1 \\tag{6}\n\nThe distance between this two margins are given by :\n\nD=\\frac{2}{\\theta}\\tag {7}\n\nImplications of the margin : larger the margin the better is the\n\n\n\n\n\n\nimages1\n\n\n\n\n\n\n\nimage2\n\n\n\n\n\n\n0.3.1 Underfitting vs Overfitting:\nDuring Training all images are not perfectly classified.\n\n\n\nFig 9 : Underfitting vs Overfitting\n\n\n\n\n\n0.4 Optimizing Perceptron :\nWhat makes a classifier good classifier: When its able to split into two groups properly. That means for a proper classified group we can have an wide margin. How do we make it a wide margin? by changing the parameter. We can change the magnitude of theta to change the width of the margin:\nLet’s create a new theta by dividing the current theta:\n\\theta^{'}=\\theta/c\nfrom 4:\n\n\\theta^T.x +\\theta_0 = 0 \\tag{4}\n\nDividing both sides by c\n\n\\frac{\\theta^T.x}{c} +\\frac{\\theta_0}{c} = 0 \\tag{5}\n\n\n\\theta^{'T}.x +\\theta_0^{'} = 0\\tag{6}\n\nThe equation of the classifier is still valid. For the margin equations:\n\n\\theta^{'T}.x +\\theta_0{'} = \\frac{-1}{c} \\tag{7}\n\n\n\\theta^{'T}.x +\\theta_0^{'} = \\frac{1}{c} \\tag{8}\n\nDistance between these two lines is\n d'=\\frac{2c}{\\theta} \\approx \\frac{\\lambda}{\\theta}\\tag{9}\n\n0.4.1 Regularisation\nThe goal is to regulate the model. This creates a wide margin and underfitting. We can increase the margin by regulating \\lambda\nThe distance on one side is \\frac{1}{||\\theta||}. The distance is maximized at \\frac{1}{||\\theta||^2}\n\n\n0.4.2 Loss Function:\nThis is the Loss when the model is over fitted during training and then it misses classification points during testing.How is this loss function calculated. we will penalize the model by assigning a loss value ( a number) so that we can nudge towards our intended solution.\n\n\n\nFig 10 : Loss Function\n\n\nLoss_h = g(y_{i}*(\\theta.x_i))\\tag{10}  \n = \\begin{cases}\n    1 & \\text{if } z&gt;1 \\\\\n    1-z & \\text{if } z&lt;=1 \\\\\n\\end{cases}\\tag{11}\n\n\n0.4.3 Optimizing Function\nJ=Loss_h + \\frac{1}{||\\theta||^2} \\tag{12}\nJ=\\sum_1^n g(y_i(\\theta.x_i)) + \\frac{1}{||\\theta||^2} \\tag{13}\nJ=\\frac{1}{n}\\sum_1^n g(y_i(\\theta.x_i+\\theta_0)) + \\frac{1}{||\\theta||^2} \\tag{14}\nJ= Average Loss + Regularization\n\n\n\n0.5 Perceptron Performance\nM times the update has been made. We know that the update formula is : \\theta_i = \\theta_{i-1} + y_i.x_i x_i = feature vector at ith round Let t_i be the times when the weights are updated. t_i \\in [ 1,.....M]\\in [1....T] to simplify two weights updates will be indicated by w_{k+1} , and previous one will be indicated by w_k\nTotal Update is Sum of Update at every step:\n\nProof\n\n\n\n\n\n\n\nDeclaration\nLogic\nCol3\n\n\n\n\nSum of Changes\n\n\n\n\n||\\sum_{t=1}^M y_t.x_t ||=\n\n\n\n\n|| \\sum_1^M  w_{k+1}-w_{k} ||=\nTheta ( or weight changes at each step)\n\n\n\n= ||\\color{red} w_{M+1} - w_{M} +\n\\color{green} w_{M} - w_{M-1} +\n\\color{magenta} w_{M-1} - w_{M-2} +\n………….\n\\color{blue} w_{3} - w_{2} +\n\\color{Aquamarine} w_{2} - w_{1} ||\nAdding individual content\n\n\n\n= ||w_{M+1} - w_1||\nBy telescoping\nw_m is a vector\n\n\n\nw_M="
  }
]